{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_words_df = pd.DataFrame(pd.read_csv('uspto_2m_keywords.tsv', sep='\\t'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_words_df = label_words_df.drop(['Title'], axis = 1)\n",
    "#label_words_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes:  633\n"
     ]
    }
   ],
   "source": [
    "#read the valid list of classes\n",
    "class_df = pd.DataFrame(pd.read_csv('ipc4_descriptions.csv', sep=','))\n",
    "class_list = list(class_df['Class'])\n",
    "print ('Number of classes: ', len(class_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "678887"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove rows with invalid labels\n",
    "label_words_df = label_words_df[~label_words_df['label'].isin(class_list)]\n",
    "len(label_words_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combineRows_column(col):\n",
    "    return col.str.cat(sep=', ')\n",
    "def removeDuplicateWords(words):\n",
    "    return set(x.strip() for x in words.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nouns = combineRows_column(label_words_df['noun'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nouns size:  84033\n"
     ]
    }
   ],
   "source": [
    "#remove duplicate nouns\n",
    "nouns = removeDuplicateWords(nouns)\n",
    "print ('nouns size: ', len(nouns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getVocabulary(words_set):\n",
    "    customize_vocabulary = {}\n",
    "    idx = 0\n",
    "    for word in words_set:\n",
    "        if (word not in customize_vocabulary):\n",
    "            customize_vocabulary[word] = idx\n",
    "            idx += 1\n",
    "    return customize_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84033"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from phrases\n",
    "customize_vocabulary = getVocabulary(nouns)\n",
    "len(customize_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_words_df = label_words_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "315000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#choose fixed number of patents randomly\n",
    "size = 500        # sample size\n",
    "replace = True  # cannot choose False, since some classes do not have 100 samples\n",
    "fn = lambda obj: obj.loc[np.random.choice(obj.index, size, replace),:]\n",
    "test_df = label_words_df.groupby('label', as_index=False).apply(fn)\n",
    "y = test_df['label']\n",
    "len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 104.99544501304626 seconds ---\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "dataset = test_df['Abstract']\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(vocabulary=customize_vocabulary, ngram_range=(1,5))\n",
    "tfidf_vectors = tfidf_vectorizer.fit_transform(dataset)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(315000, 84033)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yan/anaconda3/envs/py3-tf2/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [0 0 0 ... 0 0 0] are constant.\n",
      "  UserWarning)\n",
      "/Users/yan/anaconda3/envs/py3-tf2/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:115: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/Users/yan/anaconda3/envs/py3-tf2/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, SelectPercentile, f_classif, chi2\n",
    "\n",
    "#TOP_K = 20000\n",
    "#TOP_K = 10000\n",
    "TOP_K = 15000\n",
    "\n",
    "selector = SelectKBest(f_classif, k=min(TOP_K, tfidf_vectors.shape[1]))\n",
    "selector.fit(tfidf_vectors, y)\n",
    "tfidf_vectors = selector.transform(tfidf_vectors).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(315000, 15000)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_vectors, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 186.60967016220093 seconds ---\n",
      "Naive Bayes accuracy 0.6232539682539683\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "nb = MultinomialNB(alpha=0.001, fit_prior=False)\n",
    "model = nb.fit(X_train, y_train)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print('Naive Bayes accuracy %s' % accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.keras import models, layers, callbacks\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "\n",
    "def build_mlp_model(units, n_layers, last_layer_activation, hidden_layer_activation, input_shape, dropout_rate, numOfclasses):\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    #hidden layers\n",
    "    for _ in range(n_layers-1):\n",
    "        model.add(layers.Dense(units=units, activation=hidden_layer_activation, \n",
    "                               kernel_initializer='glorot_uniform', input_dim = input_shape))\n",
    "        model.add(layers.Dropout(rate=dropout_rate))\n",
    "    \n",
    "    #output layer\n",
    "    model.add(layers.Dense(units=numOfclasses, activation=last_layer_activation))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 252000 samples\n",
      "Epoch 1/10\n",
      "252000/252000 [==============================] - 207s 820us/sample - loss: 5.4688 - accuracy: 0.4533\n",
      "Epoch 2/10\n",
      "252000/252000 [==============================] - 207s 820us/sample - loss: 3.7360 - accuracy: 0.6652\n",
      "Epoch 3/10\n",
      "252000/252000 [==============================] - 208s 826us/sample - loss: 2.7302 - accuracy: 0.6886\n",
      "Epoch 4/10\n",
      "252000/252000 [==============================] - 222s 880us/sample - loss: 2.1535 - accuracy: 0.7067\n",
      "Epoch 5/10\n",
      "252000/252000 [==============================] - 231s 917us/sample - loss: 1.7974 - accuracy: 0.7248\n",
      "Epoch 6/10\n",
      "252000/252000 [==============================] - 217s 860us/sample - loss: 1.5571 - accuracy: 0.7402\n",
      "Epoch 7/10\n",
      "252000/252000 [==============================] - 205s 814us/sample - loss: 1.3840 - accuracy: 0.7528\n",
      "Epoch 8/10\n",
      "252000/252000 [==============================] - 207s 822us/sample - loss: 1.2533 - accuracy: 0.7639\n",
      "Epoch 9/10\n",
      "252000/252000 [==============================] - 203s 807us/sample - loss: 1.1507 - accuracy: 0.7728\n",
      "Epoch 10/10\n",
      "252000/252000 [==============================] - 205s 815us/sample - loss: 1.0678 - accuracy: 0.7808\n"
     ]
    }
   ],
   "source": [
    "#initilize parameters\n",
    "numOfclasses = len(test_df.groupby('label'))\n",
    "last_layer_activation = 'softmax'\n",
    "hidden_layer_activation = 'relu'\n",
    "input_shape = tfidf_vectors.shape[1]\n",
    "dropout_rate = 0.5\n",
    "loss = 'sparse_categorical_crossentropy'\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "n_layers = 1\n",
    "units = 30\n",
    "batch_size = 128\n",
    "\n",
    "model = build_mlp_model(units, n_layers, last_layer_activation, hidden_layer_activation, input_shape, dropout_rate, numOfclasses)\n",
    "model.compile(loss=loss, optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Add more layers, e.g., 2 or 3, the results became worse.\n",
    "\n",
    "2) Vary units with larger numbers, no affect.\n",
    "\n",
    "3) Increase the learning rate did not help a lot.\n",
    "\n",
    "4) Vary the features size from 20k to 5k, 10k, and 15k. 15k seems more effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 252000 samples, validate on 63000 samples\n",
      "Epoch 1/100\n",
      "252000/252000 [==============================] - 152s 603us/sample - loss: 5.4698 - accuracy: 0.4481 - val_loss: 4.5748 - val_accuracy: 0.5463\n",
      "Epoch 2/100\n",
      "252000/252000 [==============================] - 139s 552us/sample - loss: 3.7452 - accuracy: 0.6593 - val_loss: 3.3974 - val_accuracy: 0.5736\n",
      "Epoch 3/100\n",
      "252000/252000 [==============================] - 139s 553us/sample - loss: 2.7462 - accuracy: 0.6827 - val_loss: 2.7545 - val_accuracy: 0.5859\n",
      "Epoch 4/100\n",
      "252000/252000 [==============================] - 144s 570us/sample - loss: 2.1742 - accuracy: 0.7013 - val_loss: 2.3925 - val_accuracy: 0.5986\n",
      "Epoch 5/100\n",
      "252000/252000 [==============================] - 142s 564us/sample - loss: 1.8207 - accuracy: 0.7188 - val_loss: 2.1699 - val_accuracy: 0.6090\n",
      "Epoch 6/100\n",
      "252000/252000 [==============================] - 140s 557us/sample - loss: 1.5821 - accuracy: 0.7346 - val_loss: 2.0230 - val_accuracy: 0.6170\n",
      "Epoch 7/100\n",
      "252000/252000 [==============================] - 141s 558us/sample - loss: 1.4102 - accuracy: 0.7472 - val_loss: 1.9211 - val_accuracy: 0.6229\n",
      "Epoch 8/100\n",
      "252000/252000 [==============================] - 140s 557us/sample - loss: 1.2802 - accuracy: 0.7581 - val_loss: 1.8481 - val_accuracy: 0.6268\n",
      "Epoch 9/100\n",
      "252000/252000 [==============================] - 140s 557us/sample - loss: 1.1781 - accuracy: 0.7668 - val_loss: 1.7940 - val_accuracy: 0.6301\n",
      "Epoch 10/100\n",
      "252000/252000 [==============================] - 142s 564us/sample - loss: 1.0956 - accuracy: 0.7749 - val_loss: 1.7535 - val_accuracy: 0.6328\n",
      "Epoch 11/100\n",
      "252000/252000 [==============================] - 142s 562us/sample - loss: 1.0272 - accuracy: 0.7820 - val_loss: 1.7224 - val_accuracy: 0.6362\n",
      "Epoch 12/100\n",
      "252000/252000 [==============================] - 144s 572us/sample - loss: 0.9694 - accuracy: 0.7884 - val_loss: 1.6988 - val_accuracy: 0.6374\n",
      "Epoch 13/100\n",
      "252000/252000 [==============================] - 142s 563us/sample - loss: 0.9198 - accuracy: 0.7940 - val_loss: 1.6804 - val_accuracy: 0.6394\n",
      "Epoch 14/100\n",
      "252000/252000 [==============================] - 144s 570us/sample - loss: 0.8766 - accuracy: 0.7992 - val_loss: 1.6665 - val_accuracy: 0.6414\n",
      "Epoch 15/100\n",
      "252000/252000 [==============================] - 141s 560us/sample - loss: 0.8386 - accuracy: 0.8041 - val_loss: 1.6558 - val_accuracy: 0.6427\n",
      "Epoch 16/100\n",
      "252000/252000 [==============================] - 141s 559us/sample - loss: 0.8048 - accuracy: 0.8082 - val_loss: 1.6478 - val_accuracy: 0.6445\n",
      "Epoch 17/100\n",
      "252000/252000 [==============================] - 141s 560us/sample - loss: 0.7746 - accuracy: 0.8126 - val_loss: 1.6421 - val_accuracy: 0.6463\n",
      "Epoch 18/100\n",
      "252000/252000 [==============================] - 142s 563us/sample - loss: 0.7473 - accuracy: 0.8164 - val_loss: 1.6381 - val_accuracy: 0.6478\n",
      "Epoch 19/100\n",
      "252000/252000 [==============================] - 142s 562us/sample - loss: 0.7226 - accuracy: 0.8202 - val_loss: 1.6357 - val_accuracy: 0.6492\n",
      "Epoch 20/100\n",
      "252000/252000 [==============================] - 142s 564us/sample - loss: 0.7000 - accuracy: 0.8233 - val_loss: 1.6344 - val_accuracy: 0.6507\n",
      "Epoch 21/100\n",
      "252000/252000 [==============================] - 142s 563us/sample - loss: 0.6793 - accuracy: 0.8269 - val_loss: 1.6343 - val_accuracy: 0.6520\n",
      "Epoch 22/100\n",
      "252000/252000 [==============================] - 141s 559us/sample - loss: 0.6602 - accuracy: 0.8296 - val_loss: 1.6350 - val_accuracy: 0.6530\n",
      "Epoch 23/100\n",
      "252000/252000 [==============================] - 141s 558us/sample - loss: 0.6426 - accuracy: 0.8327 - val_loss: 1.6365 - val_accuracy: 0.6537\n",
      "Epoch 24/100\n",
      "252000/252000 [==============================] - 142s 563us/sample - loss: 0.6262 - accuracy: 0.8351 - val_loss: 1.6388 - val_accuracy: 0.6542\n",
      "Epoch 25/100\n",
      "252000/252000 [==============================] - 141s 560us/sample - loss: 0.6110 - accuracy: 0.8376 - val_loss: 1.6416 - val_accuracy: 0.6551\n",
      "Epoch 26/100\n",
      "252000/252000 [==============================] - 143s 569us/sample - loss: 0.5968 - accuracy: 0.8398 - val_loss: 1.6449 - val_accuracy: 0.6559\n",
      "Epoch 27/100\n",
      "252000/252000 [==============================] - 144s 571us/sample - loss: 0.5836 - accuracy: 0.8421 - val_loss: 1.6486 - val_accuracy: 0.6566\n",
      "Epoch 28/100\n",
      "252000/252000 [==============================] - 141s 561us/sample - loss: 0.5711 - accuracy: 0.8440 - val_loss: 1.6527 - val_accuracy: 0.6574\n",
      "Epoch 29/100\n",
      "252000/252000 [==============================] - 141s 561us/sample - loss: 0.5594 - accuracy: 0.8457 - val_loss: 1.6572 - val_accuracy: 0.6582\n",
      "Epoch 30/100\n",
      "252000/252000 [==============================] - 141s 561us/sample - loss: 0.5483 - accuracy: 0.8477 - val_loss: 1.6619 - val_accuracy: 0.6591\n",
      "Epoch 31/100\n",
      "252000/252000 [==============================] - 142s 562us/sample - loss: 0.5379 - accuracy: 0.8493 - val_loss: 1.6669 - val_accuracy: 0.6590\n"
     ]
    }
   ],
   "source": [
    "#15k features\n",
    "numOfclasses = len(test_df.groupby('label'))\n",
    "last_layer_activation = 'softmax'\n",
    "hidden_layer_activation = 'relu'\n",
    "input_shape = tfidf_vectors.shape[1]\n",
    "dropout_rate = 0.5\n",
    "loss = 'sparse_categorical_crossentropy'\n",
    "learning_rate = 0.1\n",
    "epochs = 100\n",
    "n_layers = 1\n",
    "units = 32\n",
    "batch_size = 128\n",
    "\n",
    "model = build_mlp_model(units, n_layers, last_layer_activation, hidden_layer_activation, input_shape, dropout_rate, numOfclasses)\n",
    "model.compile(loss=loss, optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "#use early-stopping\n",
    "callback_early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='auto')\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), \n",
    "                    verbose=1, callbacks=[callback_early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This experiment used a smaller dataset by considering feature selection. Only chose 5k~20k features. The classification algorithms perform worse. \n",
    "\n",
    "Applied a simple multi-layer perceptron with the filtered tfidf matrix. The result does not look too bad, since we only chose few epochs and early stopping.\n",
    "\n",
    "We also tried to vary different layers, units, batch_size, learning rate. Only the change of batch_size affected the accuracy a lot."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3-tf2]",
   "language": "python",
   "name": "conda-env-py3-tf2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
