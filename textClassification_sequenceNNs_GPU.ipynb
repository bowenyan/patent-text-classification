{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use pre-trained word embeddings based on patent data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec_model = KeyedVectors.load_word2vec_format('../input/patent-textclassification/uspto_2m_abstract_word2vec.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read text data and their labels\n",
    "label_words_df = pd.DataFrame(pd.read_csv('../input/patent-textclassification/uspto_2m_abstr_label_valid_label.tsv', sep='\\t'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Abstract</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>an apparatus for generating a saddle shaped tr...</td>\n",
       "      <td>H01L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a container or tray having various features th...</td>\n",
       "      <td>B65D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>screening methods for identifying compounds an...</td>\n",
       "      <td>A61K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>methods of forming conductive pattern structur...</td>\n",
       "      <td>H01L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a method of logging information about events f...</td>\n",
       "      <td>G07C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Abstract  label\n",
       "0  an apparatus for generating a saddle shaped tr...   H01L\n",
       "1  a container or tray having various features th...   B65D\n",
       "2  screening methods for identifying compounds an...   A61K\n",
       "3  methods of forming conductive pattern structur...   H01L\n",
       "4  a method of logging information about events f...   G07C"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_words_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "678873"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_words_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "630000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set up balanced dataset\n",
    "size = 1000       # sample size\n",
    "replace = True  # cannot choose False, since some classes do not have 100 samples\n",
    "fn = lambda obj: obj.loc[np.random.choice(obj.index, size, replace),:]\n",
    "sub_df = label_words_df.groupby('label', as_index=False).apply(fn)\n",
    "y = sub_df['label']\n",
    "len(sub_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Abstract', 'label'], dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens:  76519\n"
     ]
    }
   ],
   "source": [
    "#extract words\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "#tokenizer = Tokenizer(num_words=20)\n",
    "tokenizer.fit_on_texts(sub_df['Abstract'].values)\n",
    "sequences = tokenizer.texts_to_sequences(sub_df['Abstract'].values)\n",
    "\n",
    "\n",
    "#max length for pad sequences\n",
    "seq_max_len = max([len(s.split()) for s in sub_df['Abstract'].values])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print ('Number of unique tokens: ', len(word_index))\n",
    "\n",
    "x_pad = pad_sequences(sequences, maxlen=seq_max_len)\n",
    "encoder = LabelEncoder()\n",
    "y_set = encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_dim = word2vec_model.vector_size\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec_model:\n",
    "        embedding_matrix[i] = word2vec_model[word]\n",
    "    #print (word, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LSTM, GRU, CNN with the pre-trained embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.layers import Embedding, Dense, LSTM, GRU\n",
    "from tensorflow.python.keras.layers import Conv1D, MaxPooling1D, Flatten\n",
    "from tensorflow.python.keras.layers import Dropout, SeparableConv1D, GlobalAveragePooling1D\n",
    "from tensorflow.python.keras import models, callbacks\n",
    "from tensorflow.python.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numOfclasses = len(set(y_set))\n",
    "last_layer_activation = 'softmax'\n",
    "hidden_layer_activation = 'relu'\n",
    "loss = 'sparse_categorical_crossentropy'\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_pad, y_set, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#use grid search to choose best options\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "\n",
    "def create_model(filters, kernel_size, pool_size):\n",
    "    model = models.Sequential()\n",
    "    model.add(Embedding(len(word_index) + 1, embedding_dim, \n",
    "                        weights=[embedding_matrix], \n",
    "                        input_length=seq_max_len, trainable=False))\n",
    "    model.add(Conv1D(filters, kernel_size, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=pool_size))  #vary 2,3,4,5\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=numOfclasses, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    #history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), verbose=1)\n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, verbose=1)\n",
    "\n",
    "numOfclasses = len(set(y_set))\n",
    "pool_size = [2, 3, 4]\n",
    "features_list = [64, 128, 256]\n",
    "kernel_size = [3, 5, 7]\n",
    "batch_size = [32, 64, 128]\n",
    "epochs = [1]\n",
    "\n",
    "param_grid = dict(batch_size=batch_size,\n",
    "                  epochs=epochs,\n",
    "                  pool_size=pool_size,\n",
    "                 filters=features_list,\n",
    "                 kernel_size=kernel_size)\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)\n",
    "grid_result = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "#{'batch_size': 128, 'epochs': 1, 'filters': 256, 'kernel_size': 3, 'pool_size': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 504000 samples, validate on 126000 samples\n",
      "Epoch 1/5\n",
      "504000/504000 [==============================] - 165s 327us/sample - loss: 2.3238 - accuracy: 0.5561 - val_loss: 1.5664 - val_accuracy: 0.6733\n",
      "Epoch 2/5\n",
      "504000/504000 [==============================] - 159s 316us/sample - loss: 1.0634 - accuracy: 0.7652 - val_loss: 1.4292 - val_accuracy: 0.6981\n",
      "Epoch 3/5\n",
      "504000/504000 [==============================] - 156s 310us/sample - loss: 0.8176 - accuracy: 0.8052 - val_loss: 1.3797 - val_accuracy: 0.7058\n",
      "Epoch 4/5\n",
      "504000/504000 [==============================] - 156s 309us/sample - loss: 0.7000 - accuracy: 0.8198 - val_loss: 1.3573 - val_accuracy: 0.7079\n",
      "Epoch 5/5\n",
      "504000/504000 [==============================] - 157s 311us/sample - loss: 0.6262 - accuracy: 0.8273 - val_loss: 1.3708 - val_accuracy: 0.7086\n"
     ]
    }
   ],
   "source": [
    "#adding more layers get worse results\n",
    "epochs = 5\n",
    "model = models.Sequential()\n",
    "model.add(Embedding(len(word_index) + 1, embedding_dim, weights=[embedding_matrix], \n",
    "                    input_length=seq_max_len, trainable=False))\n",
    "model.add(Conv1D(256, 3, activation=hidden_layer_activation))\n",
    "model.add(MaxPooling1D(pool_size=2))  #vary 2,3,4,5\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=numOfclasses, activation=last_layer_activation))\n",
    "model.compile(loss=loss, optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increased 5%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Train CNN and allows to learn the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_kg_hide-output": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 504000 samples, validate on 126000 samples\n",
      "Epoch 1/2\n",
      "504000/504000 [==============================] - 484s 961us/sample - loss: 2.6676 - accuracy: 0.5119 - val_loss: 1.7155 - val_accuracy: 0.6529\n",
      "Epoch 2/2\n",
      "504000/504000 [==============================] - 485s 963us/sample - loss: 1.2232 - accuracy: 0.7404 - val_loss: 1.5144 - val_accuracy: 0.6893\n"
     ]
    }
   ],
   "source": [
    "#CNN without weights at the embedding layer\n",
    "epochs = 2\n",
    "model = models.Sequential()\n",
    "model.add(Embedding(len(word_index) + 1, embedding_dim, input_length=seq_max_len))\n",
    "model.add(Conv1D(256, 3, activation=hidden_layer_activation))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=numOfclasses, activation=last_layer_activation))\n",
    "model.compile(loss=loss, optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3-tf2]",
   "language": "python",
   "name": "conda-env-py3-tf2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
