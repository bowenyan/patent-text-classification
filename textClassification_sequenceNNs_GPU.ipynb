{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.models import KeyedVectors\nimport pandas as pd\nimport numpy as np","execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Processing"},{"metadata":{},"cell_type":"markdown","source":"### Use pre-trained word embeddings based on patent data"},{"metadata":{"trusted":true},"cell_type":"code","source":"word2vec_model = KeyedVectors.load_word2vec_format('../input/patent-textclassification/uspto_2m_abstract_word2vec.bin', binary=True)","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#read text data and their labels\nlabel_words_df = pd.DataFrame(pd.read_csv('../input/patent-textclassification/uspto_2m_abstr_label_valid_label.tsv', sep='\\t'))","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_words_df.head()","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"                                            Abstract  label\n0  an apparatus for generating a saddle shaped tr...   H01L\n1  a container or tray having various features th...   B65D\n2  screening methods for identifying compounds an...   A61K\n3  methods of forming conductive pattern structur...   H01L\n4  a method of logging information about events f...   G07C","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Abstract</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>an apparatus for generating a saddle shaped tr...</td>\n      <td>H01L</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>a container or tray having various features th...</td>\n      <td>B65D</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>screening methods for identifying compounds an...</td>\n      <td>A61K</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>methods of forming conductive pattern structur...</td>\n      <td>H01L</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>a method of logging information about events f...</td>\n      <td>G07C</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(label_words_df)","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"678873"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#set up balanced dataset\nsize = 1000       # sample size\nreplace = True  # cannot choose False, since some classes do not have 100 samples\nfn = lambda obj: obj.loc[np.random.choice(obj.index, size, replace),:]\nsub_df = label_words_df.groupby('label', as_index=False).apply(fn)\ny = sub_df['label']\nlen(sub_df)","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"630000"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.columns","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"Index(['Abstract', 'label'], dtype='object')"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#extract words\nfrom tensorflow.python.keras.preprocessing.text import Tokenizer\nfrom tensorflow.python.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.preprocessing import LabelEncoder\n\ntokenizer = Tokenizer()\n#tokenizer = Tokenizer(num_words=20)\ntokenizer.fit_on_texts(sub_df['Abstract'].values)\nsequences = tokenizer.texts_to_sequences(sub_df['Abstract'].values)\n\n\n#max length for pad sequences\nseq_max_len = max([len(s.split()) for s in sub_df['Abstract'].values])\n\nword_index = tokenizer.word_index\nprint ('Number of unique tokens: ', len(word_index))\n\nx_pad = pad_sequences(sequences, maxlen=seq_max_len)\nencoder = LabelEncoder()\ny_set = encoder.fit_transform(y)","execution_count":8,"outputs":[{"output_type":"stream","text":"Number of unique tokens:  76519\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Prepare the embedding layer"},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_dim = word2vec_model.vector_size\nembedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\nfor word, i in word_index.items():\n    if word in word2vec_model:\n        embedding_matrix[i] = word2vec_model[word]\n    #print (word, i)","execution_count":9,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train LSTM, GRU, CNN with the pre-trained embedding layer"},{"metadata":{},"cell_type":"markdown","source":"CNN is faster than LSTM, GRU.\n\nFuture work:\n\n1. Try seqCNN, etc., other CNN variations.\n2. Try large dataset using CNN, compared to Naive Bayes, SVM."},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.python.keras.layers import Embedding, Dense, LSTM, GRU\nfrom tensorflow.python.keras.layers import Conv1D, MaxPooling1D, Flatten\nfrom tensorflow.python.keras.layers import Dropout, SeparableConv1D, GlobalAveragePooling1D\nfrom tensorflow.python.keras import models, callbacks\nfrom tensorflow.python.keras.optimizers import Adam","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numOfclasses = len(set(y_set))\nlast_layer_activation = 'softmax'\nhidden_layer_activation = 'relu'\nloss = 'sparse_categorical_crossentropy'\nbatch_size = 128","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x_pad, y_set, test_size = 0.2, random_state = 0)","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#use grid search to choose best options\nfrom sklearn.model_selection import GridSearchCV\nfrom keras.wrappers.scikit_learn import KerasClassifier\n\n\ndef create_model(filters, kernel_size, pool_size):\n    model = models.Sequential()\n    model.add(Embedding(len(word_index) + 1, embedding_dim, \n                        weights=[embedding_matrix], \n                        input_length=seq_max_len, trainable=False))\n    model.add(Conv1D(filters, kernel_size, activation='relu'))\n    model.add(MaxPooling1D(pool_size=pool_size))  #vary 2,3,4,5\n    model.add(Flatten())\n    model.add(Dense(units=numOfclasses, activation='softmax'))\n    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    #history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), verbose=1)\n    return model\n\nmodel = KerasClassifier(build_fn=create_model, verbose=1)\n\nnumOfclasses = len(set(y_set))\npool_size = [2, 3, 4]\nfeatures_list = [64, 128, 256]\nkernel_size = [3, 5, 7]\nbatch_size = [32, 64, 128]\nepochs = [1]\n\nparam_grid = dict(batch_size=batch_size,\n                  epochs=epochs,\n                  pool_size=pool_size,\n                 filters=features_list,\n                 kernel_size=kernel_size)\n\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)\ngrid_result = grid.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n#{'batch_size': 128, 'epochs': 1, 'filters': 256, 'kernel_size': 3, 'pool_size': 2}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#adding more layers get worse results\nepochs = 5\nmodel = models.Sequential()\nmodel.add(Embedding(len(word_index) + 1, embedding_dim, weights=[embedding_matrix], \n                    input_length=seq_max_len, trainable=False))\nmodel.add(Conv1D(256, 3, activation=hidden_layer_activation))\nmodel.add(MaxPooling1D(pool_size=2))  #vary 2,3,4,5\nmodel.add(Flatten())\nmodel.add(Dense(units=numOfclasses, activation=last_layer_activation))\nmodel.compile(loss=loss, optimizer='adam', metrics=['accuracy'])\nhistory = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), verbose=1)","execution_count":13,"outputs":[{"output_type":"stream","text":"Train on 504000 samples, validate on 126000 samples\nEpoch 1/5\n504000/504000 [==============================] - 165s 327us/sample - loss: 2.3238 - accuracy: 0.5561 - val_loss: 1.5664 - val_accuracy: 0.6733\nEpoch 2/5\n504000/504000 [==============================] - 159s 316us/sample - loss: 1.0634 - accuracy: 0.7652 - val_loss: 1.4292 - val_accuracy: 0.6981\nEpoch 3/5\n504000/504000 [==============================] - 156s 310us/sample - loss: 0.8176 - accuracy: 0.8052 - val_loss: 1.3797 - val_accuracy: 0.7058\nEpoch 4/5\n504000/504000 [==============================] - 156s 309us/sample - loss: 0.7000 - accuracy: 0.8198 - val_loss: 1.3573 - val_accuracy: 0.7079\nEpoch 5/5\n504000/504000 [==============================] - 157s 311us/sample - loss: 0.6262 - accuracy: 0.8273 - val_loss: 1.3708 - val_accuracy: 0.7086\n","name":"stdout"}]},{"metadata":{"collapsed":true},"cell_type":"markdown","source":"# Train CNN and allows to learn the embedding layer"},{"metadata":{"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"#CNN without weights at the embedding layer\nepochs = 2\nmodel = models.Sequential()\nmodel.add(Embedding(len(word_index) + 1, embedding_dim, input_length=seq_max_len))\nmodel.add(Conv1D(256, 3, activation=hidden_layer_activation))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(Flatten())\nmodel.add(Dense(units=numOfclasses, activation=last_layer_activation))\nmodel.compile(loss=loss, optimizer='adam', metrics=['accuracy'])\nhistory = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), verbose=1)","execution_count":14,"outputs":[{"output_type":"stream","text":"Train on 504000 samples, validate on 126000 samples\nEpoch 1/2\n504000/504000 [==============================] - 484s 961us/sample - loss: 2.6676 - accuracy: 0.5119 - val_loss: 1.7155 - val_accuracy: 0.6529\nEpoch 2/2\n504000/504000 [==============================] - 485s 963us/sample - loss: 1.2232 - accuracy: 0.7404 - val_loss: 1.5144 - val_accuracy: 0.6893\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#seqCNN\ndropout_rate = 0.2\nepochs = 1\nmodel = models.Sequential()\nmodel.add(Embedding(len(word_index) + 1, embedding_dim, input_length=seq_max_len))\nmodel.add(Dropout(rate=dropout_rate))\nmodel.add(SeparableConv1D(filters=256,\n                          kernel_size=3,\n                          activation='relu',\n                          bias_initializer='random_uniform',\n                          depthwise_initializer='random_uniform',\n                          padding='same'))\nmodel.add(SeparableConv1D(filters=256,\n                          kernel_size=3,\n                          activation='relu',\n                          bias_initializer='random_uniform',\n                          depthwise_initializer='random_uniform',\n                          padding='same'))\nmodel.add(MaxPooling1D(pool_size=2))\n\nmodel.add(SeparableConv1D(filters=256 * 2,\n                          kernel_size=3,\n                          activation='relu',\n                          bias_initializer='random_uniform',\n                          depthwise_initializer='random_uniform',\n                          padding='same'))\nmodel.add(SeparableConv1D(filters=256 * 2,\n                          kernel_size=3,\n                          activation='relu',\n                          bias_initializer='random_uniform',\n                          depthwise_initializer='random_uniform',\n                          padding='same'))\nmodel.add(GlobalAveragePooling1D())\nmodel.add(Dropout(rate=dropout_rate))\nmodel.add(Dense(units=numOfclasses, activation=last_layer_activation))\nmodel.compile(loss=loss, optimizer='adam', metrics=['accuracy'])\nhistory = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), verbose=1)","execution_count":17,"outputs":[{"output_type":"stream","text":"Train on 504000 samples, validate on 126000 samples\n504000/504000 [==============================] - 653s 1ms/sample - loss: 6.4466 - accuracy: 0.0014 - val_loss: 6.4466 - val_accuracy: 0.0014\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# CNN+LSTM\nepochs = 2\nmodel = models.Sequential()\nmodel.add(Embedding(len(word_index) + 1, embedding_dim, weights=[embedding_matrix], \n                    input_length=seq_max_len, trainable=False))\nmodel.add(Conv1D(256, 3, activation=hidden_layer_activation))\nmodel.add(MaxPooling1D(pool_size=2))  #vary 2,3,4,5\nmodel.add(Flatten())\nmodel.add(LSTM(units=128, dropout=dropout_rate, recurrent_dropout=dropout_rate))\nmodel.add(Dense(units=numOfclasses, activation=last_layer_activation))\nmodel.compile(loss=loss, optimizer='adam', metrics=['accuracy'])\nhistory = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), verbose=1)         "}],"metadata":{"kernelspec":{"display_name":"Python [conda env:py3-tf2]","language":"python","name":"conda-env-py3-tf2-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":1}